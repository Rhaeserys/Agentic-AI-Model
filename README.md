# Agentic-AI-Model
Developing an agentic AI program that can perceive its environment, plan, reason, and take action autonomously to achieve a goalâ€”involves a progression from conceptual design to deployment.

11/06/2025

Here is a detailed breakdown of your agentic AI model, including its purpose, required installations, and common challenges you might face when using the crewai framework.

ðŸ¤– Description of Your Agentic AI Model
This script creates a sophisticated Agentic AI system using the crewai framework. It's designed to emulate a collaborative team of specialized analysts to produce a comprehensive report on the Oil & Gas midstream sector.

Hereâ€™s how it works:

Core Engine (LLM): The "brain" for every agent is Google's Gemini 1.5 Pro, which is instantiated using crewai's native LLM class.

The "Crew": You have defined a crew of four distinct agents, each with a specific role, goal, and set of tools:

market_analyst: Finds profitable markets by analyzing regional data.

operations_analyst: Monitors the physical flow of O&G (refining, logistics, storage).

financial_analyst: Tracks financial benchmarks like the Henry Hub spot price.

chief_strategist: The "manager" agent. This agent is the only one with allow_delegation=True and access to the search_tool. Its job is to synthesize the reports from the other three agents and combine them with live web data (competitor news) to create a final, predictive report.

Tools (Capabilities): You've equipped your agents with two types of tools:

Pre-built Tool: SerperDevTool allows the chief_strategist to search the internet for real-time information.

Custom Tools: You defined multiple functions like refining_analyzer_tool and market_demand_tool using the @tool decorator. These tools allow your agents to perform specific, custom actions, such as reading and processing a local CSV file with pandas or (in this script) pulling mock data.

Process (Workflow): The crew operates on a Process.sequential basis. This means Task 1 (task_market_analysis) runs, then Task 2 (task_operations_analysis), and so on. The final task_final_report is given the context (i.e., the results) from the previous three tasks, which the chief_strategist uses to build its summary.

In short, you've built an automated analysis team that can gather data from local files, access mock data, search the web, and collaborate to produce a single, synthesized report.

ðŸ“¦ Important Pip Installs
To run this code, a user will need to install several Python libraries. The imports in your script imply the following are necessary:

Bash

# The core framework for creating agents and crews
pip install crewai

# The library for pre-built tools like Serper
pip install crewai-tools

# Required for your custom refining_analyzer_tool to read and process the file
pip install pandas

# The underlying library to connect to Google's Gemini API
pip install google-generativeai
âš ï¸ Important: Cloud API & Subscription Requirement
This is the most critical point for any user to understand before running this script:

This model is NOT free to run.

It relies on external, cloud-based services that require paid subscriptions and API keys. You must have active, funded accounts for the following services and set them as environment variables:

Google AI (Gemini):

Variable: GOOGLE_API_KEY

Service: This key is for Google's Gemini 1.5 Pro model, which is a paid, usage-based API. Every time an agent thinks, plans, or writes a response, it makes an API call, which will incur a cost. A complex 4-agent crew like this one can make many calls in a single run.

Serper:

Variable: SERPER_API_KEY

Service: This is a third-party, paid subscription service for Google Search results. It is used by the chief_strategist agent to perform web searches.

Without valid API keys for both of these services, the script will fail.

ðŸ”§ Common Troubles & Pitfalls with CrewAI
When you or another user starts building with crewai, you will likely run into a few common challenges. Here are some of the most frequent ones to watch for:

1. API Key & Environment Variable Errors:

The Problem: The most common failure is a MissingEnvironmentVariable error or a 401/403 authentication error from the API.

The Fix: Ensure your .env file (if you use one) is being loaded correctly, or that you've set the os.environ variables before you instantiate any tools or LLMs that need them. Double-check that the keys are valid and have billing enabled.

2. Custom Tool Failures:

The Problem: Your custom @tool functions are a major weak point. In your script, the refining_analyzer_tool hard-codes the file name PET_SUM_SNDW_DCUS_NUS_4.xls - Data 2.csv. If this file is not in the exact same directory where the script is run, the tool will fail with a FileNotFoundError, and the agent will crash.

The Fix: Use absolute paths, or better yet, pass the file path as an argument to the tool. You should also add robust try...except blocks inside your tools (as you have done) to return a helpful error message to the agent instead of crashing the program.

3. Agent "Hallucination" or Tool Misuse:

The Problem: An agent might not use a tool when it should, use the wrong tool, or pass the wrong arguments. For example, an agent might try to answer a question from its own knowledge instead of using your refining_analyzer_tool to get the real-time data.

The Fix: This is the hardest part of agentic design. The fix is to:

Write better tool descriptions: The agent reads the docstring (like "Analyzes U.S. refinery operations.") to decide when to use a tool. Be extremely clear and descriptive.

Write better task descriptions: Your expected_output in the task definition is crucial for guiding the agent.

Make agents simpler: Don't give one agent 10 tools. Give it 1-2 highly relevant tools.

4. Framework Version Conflicts:

The Problem: crewai and the AI landscape move very fast. A script that works today might break tomorrow after a pip install --upgrade crewai. A new version might change how LLMs are imported, how Context is handled, or how Tools are defined.

The Fix: When you have a working script, lock your dependencies using a requirements.txt file (e.g., pip freeze > requirements.txt). This saves the exact versions (crewai==0.30.11, etc.) so you can recreate the environment perfectly.

5. "Lazy" Agents or Poor Synthesis:

The Problem: Sometimes, a final agent (like your chief_strategist) will just staple the reports from other agents together without adding any real insight. Or, an agent will give a very short, "lazy" answer.

The Fix: This is a "prompt engineering" problem. You need to improve the agent's goal and backstory to be more demanding. For your chief_strategist, you could add to its goal: "Do not just list the other reports. You MUST synthesize them and provide three new, predictive insights based on their combined data."

*** The Model still needs a bit more work, but will update soon. ***
